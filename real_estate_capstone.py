# -*- coding: utf-8 -*-
"""Real_Estate_Capstone.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1y8Fejc57ISYWBtIctQwMDtleooKJteqo

# Real Estate

**`Problem Statement`**
1.   A banking institution requires actionable insights into mortgage-backed securities, geographic business investment, and real estate analysis. 
2.   The mortgage bank would like to identify potential monthly mortgage expenses for each region based on monthly family income and rental of the real estate
1.   A statistical model needs to be created to predict the potential demand in dollars amount of loan for each of the region in the USA. Also, there is a need to create a dashboard which would refresh periodically post data retrieval from the agencies.
2.   The dashboard must demonstrate relationships and trends for the key metrics as follows: number of loans, average rental income, monthly mortgage and owner’s cost, family income vs mortgage cost comparison across different regions. The metrics described here do not limit the dashboard to these few.

# Importing Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from pandas.plotting import scatter_matrix
# %matplotlib inline

import seaborn as sns
sns.set(style="white", color_codes=True)
sns.set(font_scale=1.5)

pd.options.display.max_columns = 999
pd.options.display.max_rows = 999

"""#Project Task: Week 1

##Data Import and Preparation:
"""



"""**`Data Import and Preparation`**
1. Import data.
2. Figure out the primary key and look for the requirement of indexing.
3. Gauge the fill rate of the variables and devise plans for missing value treatment. Please explain explicitly the reason for the treatment chosen for each variable.

**`Exploratory Data Analysis (EDA)`**
4. Perform debt analysis. You may take the following steps:
   1. Explore the top 2,500 locations where the percentage of households with a second mortgage is the highest and percent ownership is above 10 percent. Visualize using geo-map. You may keep the upper limit for the percent of households with a second mortgage to 50 percent

  2. Use the following bad debt equation: Bad Debt = P (Second Mortgage ∩ Home Equity Loan) Bad Debt = second_mortgage + home_equity - home_equity_second_mortgage 
  
  3. Create pie charts to show overall debt and bad debt

  4. Create Box and whisker plot and analyze the distribution for 2nd mortgage, home equity, good debt, and bad debt for different cities

  5. Create a collated income distribution chart for family income, house hold income, and remaining income

### Importing Dataset

**`Dataset Description`**

Following are the themes the fields fall under Home Owner Costs: Sum of utilities, property taxes.

1. Second Mortgage: Households with a second mortgage statistics.

2. Home Equity Loan: Households with a Home equity Loan statistics.

3.	Debt: Households with any type of debt statistics.

4.	Mortgage Costs: Statistics regarding mortgage payments, home equity loans, utilities and property taxes

5.	Home Owner Costs: Sum of utilities, property taxes statistics

6.	Gross Rent: Contract rent plus the estimated average monthly cost of utility features

7.	Gross Rent as Percent of Income Gross rent as the percent of income very interesting

8.	High school Graduation: High school graduation statistics.

9.	Population Demographics: Population demographic statistics.

10. Age Demographics: Age demographic statistics.

11. Household Income: Total income of people residing in the household.

12. Family Income: Total income of people related to the householder.
"""

train_DF=pd.read_csv("train.csv")
test_DF=pd.read_csv("test.csv")

"""### Properties Of Dataset"""

train_DF.shape

train_DF.columns

train_DF.info()
#float64(68), int64(6), object(6)

#Null Count
null_count = pd.DataFrame(data = train_DF.isna().sum()).reset_index()
null_count.columns = ['Columns', 'Count']
null_count

train_DF.describe()

train_DF.head()

"""### Figure out the primary key and look for the requirement of indexing."""

#len(train_DF['UID'].unique())

#UID is unique userID value in the train and test dataset. So an index can be created from the UID feature
train_DF.set_index(keys=['UID'],inplace=True)#Set the DataFrame index using existing columns.
test_DF.set_index(keys=['UID'],inplace=True)

"""### Gauge the fill rate of the variables and devise plans for missing value treatment. Please explain explicitly the reason for the treatment chosen for each variable."""

#percantage of missing values in train set
missing_list_train=train_DF.isnull().sum() *100/len(train_DF)
missing_values_train_DF=pd.DataFrame(missing_list_train,columns=['Percantage of missing values'])
missing_values_train_DF.sort_values(by=['Percantage of missing values'],inplace=True,ascending=False)
missing_values_train_DF[missing_values_train_DF['Percantage of missing values'] >0][:10]

#percantage of missing values in test set
missing_list_test=test_DF.isnull().sum() *100/len(train_DF)
missing_values_test_DF=pd.DataFrame(missing_list_test,columns=['Percantage of missing values'])
missing_values_test_DF.sort_values(by=['Percantage of missing values'],inplace=True,ascending=False)
missing_values_test_DF[missing_values_test_DF['Percantage of missing values'] >0][:10]

train_Variance = pd.DataFrame(train_DF.var(),columns=['Variance'])
train_Variance_sort = train_Variance.sort_values(by=['Variance'],ascending=True)
train_Variance_sort

test_Variance = pd.DataFrame(test_DF.var(),columns=['Variance'])
test_Variance_sort = test_Variance.sort_values(by=['Variance'],ascending=True)
test_Variance_sort

train_DF.drop(columns=['BLOCKID','SUMLEVEL'],inplace=True) #SUMLEVEL doest not have any predictive power and no variance

test_DF.drop(columns=['BLOCKID','SUMLEVEL'],inplace=True) #SUMLEVEL doest not have any predictive power

# Imputing  missing values with mean
missing_train_cols=[]
for col in train_DF.columns:
    if train_DF[col].isna().sum() !=0:
         missing_train_cols.append(col)
print(missing_train_cols)

# Missing cols are all numerical variables
for col in train_DF.columns:
    if col in (missing_train_cols):
        train_DF[col].replace(np.nan, train_DF[col].mean(),inplace=True)

# Imputing  missing values with mean
missing_test_cols=[]
for col in test_DF.columns:
    if test_DF[col].isna().sum() !=0:
         missing_test_cols.append(col)
print(missing_test_cols)

# Missing cols are all numerical variables
for col in test_DF.columns:
    if col in (missing_test_cols):
        test_DF[col].replace(np.nan, test_DF[col].mean(),inplace=True)

train_DF.isna().sum().sum()

test_DF.isna().sum().sum()

"""##Exploratory Data Analysis (EDA):
4.Perform debt analysis. You may take the following steps:

### 1.Explore the top 2,500 locations where the percentage of households with a second mortgage is the highest and percent ownership is above 10 percent. Visualize using geo-map. You may keep the upper limit for the percent of households with a second mortgage to 50 percent
"""

!pip install pandasql
from pandasql import sqldf
q1 = "select place,pct_own,second_mortgage,lat,lng from train_DF where pct_own >0.10 and second_mortgage <0.5 order by second_mortgage DESC LIMIT 2500;"
pysqldf = lambda q: sqldf(q, globals())#The main function used in pandasql is sqldf. sqldf accepts 2 parametrs 
                                        #1. a sql query string = q1
                                        #2. an set of session/environment variables (locals() or globals())
train_DF_location_mort_pct=pysqldf(q1)

train_DF_location_mort_pct.head()

#!pip install plotly
import plotly.express as px
import plotly.graph_objects as go

fig = go.Figure(data=go.Scattergeo(
    lat = train_DF_location_mort_pct['lat'],
    lon = train_DF_location_mort_pct['lng']),
    )
fig.update_layout(
    geo=dict(
        scope = 'north america',
        showland = True,
        landcolor = "rgb(212, 212, 212)",
        subunitcolor = "rgb(255, 255, 255)",
        countrycolor = "rgb(255, 255, 255)",
        showlakes = True,
        lakecolor = "rgb(255, 255, 255)",
        showsubunits = True,
        showcountries = True,
        resolution = 50,
        projection = dict(
            type = 'conic conformal',
            rotation_lon = -100
        ),
        lonaxis = dict(
            showgrid = True,
            gridwidth = 0.5,
            range= [ -140.0, -55.0 ],
            dtick = 5
        ),
        lataxis = dict (
            showgrid = True,
            gridwidth = 0.5,
            range= [ 20.0, 60.0 ],
            dtick = 5
        )
    ),
    title='Top 2,500 locations with second mortgage is the highest and percent ownership is above 10 percent')
fig.show()

"""###2.Use the following bad debt equation: Bad Debt = P (Second Mortgage ∩ Home Equity Loan) Bad Debt = second_mortgage + home_equity - home_equity_second_mortgage """

train_DF['bad_debt']=train_DF['second_mortgage']+train_DF['home_equity']-train_DF['home_equity_second_mortgage']

"""### 3.Create pie charts to show overall debt and bad debt"""

train_DF['bins'] = pd.cut(train_DF['bad_debt'],bins=[0,0.10,1], labels=["less than 50%","50-100%"])
train_DF.groupby(['bins']).size().plot(kind='pie',subplots=True,startangle=90, autopct='%1.1f%%')
plt.axis('equal')

plt.show()
#df.plot.pie(subplots=True,figsize=(8, 3))

"""###4.Create Box and whisker plot and analyze the distribution for 2nd mortgage, home equity, good debt, and bad debt for different cities"""

cols=[]
train_DF.columns

#Taking Hamilton and Manhattan cities data
cols=['second_mortgage','home_equity','debt','bad_debt']
df_box_hamilton=train_DF.loc[train_DF['city'] == 'Hamilton']
df_box_manhattan=train_DF.loc[train_DF['city'] == 'Manhattan']
df_box_city=pd.concat([df_box_hamilton,df_box_manhattan])
df_box_city.head(4)

plt.figure(figsize=(10,5))
sns.boxplot(data=df_box_city,x='second_mortgage', y='city',width=0.5,palette="Set3")
plt.show()

plt.figure(figsize=(10,5))
sns.boxplot(data=df_box_city,x='home_equity', y='city',width=0.5,palette="Set3")
plt.show()

plt.figure(figsize=(10,5))
sns.boxplot(data=df_box_city,x='debt', y='city',width=0.5,palette="Set3")
plt.show()

plt.figure(figsize=(10,5))
sns.boxplot(data=df_box_city,x='bad_debt', y='city',width=0.5,palette="Set3")
plt.show()

"""###5.Create a collated income distribution chart for family income, house hold income, and remaining income"""

sns.distplot(train_DF['hi_mean'])
plt.title('Household income distribution chart')
plt.show()

sns.distplot(train_DF['family_mean'])
plt.title('Family income distribution chart')
plt.show()

sns.distplot(train_DF['family_mean']-train_DF['hi_mean'])
plt.title('Remaining income distribution chart')
plt.show()

"""#Project Task: Week 2

##Exploratory Data Analysis (EDA):

###1. Perform EDA and come out with insights into population density and age. You may have to derive new fields (make sure to weight averages for accurate measurements):
"""

#plt.figure(figsize=(25,10))
fig,(ax1,ax2,ax3)=plt.subplots(3,1)
sns.distplot(train_DF['pop'],ax=ax1)
sns.distplot(train_DF['male_pop'],ax=ax2)
sns.distplot(train_DF['female_pop'],ax=ax3)
plt.subplots_adjust(wspace=0.8,hspace=0.8)
plt.tight_layout()
plt.show()

#plt.figure(figsize=(25,10))
fig,(ax1,ax2)=plt.subplots(2,1)
sns.distplot(train_DF['male_age_mean'],ax=ax1)
sns.distplot(train_DF['female_age_mean'],ax=ax2)
plt.subplots_adjust(wspace=0.8,hspace=0.8)
plt.tight_layout()
plt.show()

"""a) Use pop and ALand variables to create a new field called population density"""

train_DF['pop_density']=train_DF['pop']/train_DF['ALand']

test_DF['pop_density']=test_DF['pop']/test_DF['ALand']

sns.distplot(train_DF['pop_density'])
plt.title('Population Density')
plt.show() # Very less density is noticed

"""b) Use male_age_median, female_age_median, male_pop, and female_pop to create a new field called median age"""

train_DF['age_median']=(train_DF['male_age_median']+train_DF['female_age_median'])/2
test_DF['age_median']=(test_DF['male_age_median']+test_DF['female_age_median'])/2

train_DF[['male_age_median','female_age_median','male_pop','female_pop','age_median']].head()

"""c) Visualize the findings using appropriate chart type"""

sns.distplot(train_DF['age_median'])
plt.title('Median Age')
plt.show()
# Age of population is mostly between 20 and 60
# Majority are of age around 40
# Median age distribution has a gaussian distribution
# Some right skewness is noticed

sns.boxplot(train_DF['age_median'])
plt.title('Population Density')
plt.show()

"""###2. Create bins for population into a new variable by selecting appropriate class interval so that the number of categories don’t exceed 5 for the ease of analysis."""

train_DF['pop'].describe()

train_DF['pop_bins']=pd.cut(train_DF['pop'],bins=5,labels=['very low','low','medium','high','very high'])

train_DF[['pop','pop_bins']]

train_DF['pop_bins'].value_counts()

"""a) Analyze the married, separated, and divorced population for these population brackets"""

train_DF.groupby(by='pop_bins')[['married','separated','divorced']].count()

train_DF.groupby(by='pop_bins')[['married','separated','divorced']].agg(["mean", "median"])

"""b) Visualize using appropriate chart type"""

plt.figure(figsize=(10,5))
pop_bin_married=train_DF.groupby(by='pop_bins')[['married','separated','divorced']].agg(["mean"])
pop_bin_married.plot(figsize=(20,8))
plt.legend(loc='best')
plt.show()

"""###3. Please detail your observations for rent as a percentage of income at an overall level, and for different states."""

rent_state_mean=train_DF.groupby(by='state')['rent_mean'].agg(["mean"])
rent_state_mean.head()

income_state_mean=train_DF.groupby(by='state')['family_mean'].agg(["mean"])
income_state_mean.head()

rent_perc_of_income=rent_state_mean['mean']/income_state_mean['mean']
rent_perc_of_income.head(10)

#overall level rent as a percentage of income
sum(train_DF['rent_mean'])/sum(train_DF['family_mean'])

"""###4. Perform correlation analysis for all the relevant variables by creating a heatmap. Describe your findings."""

train_DF.columns

cor=train_DF[['COUNTYID','STATEID','zip_code','type','pop', 'family_mean',
         'second_mortgage', 'home_equity', 'debt','hs_degree',
           'age_median','pct_own', 'married','separated', 'divorced']].corr()

plt.figure(figsize=(20,10))
sns.heatmap(cor,annot=True,cmap='coolwarm')
plt.show()

"""#Project Task: Week 3

##Data Pre-processing:

1. The economic multivariate data has a significant number of measured variables. The goal is to find where the measured variables depend on a number of smaller unobserved common factors or latent variables. 
2. Each variable is assumed to be dependent upon a linear combination of the common factors, and the coefficients are known as loadings. Each measured variable also includes a component due to independent      random variability, known as “specific variance” because it is specific to one variable. Obtain the common factors and then plot the loadings. Use factor analysis to find latent variables in our dataset and gain          insight into the linear relationships in the data. Following are the list of latent variables:

• Highschool graduation rates

• Median population age

• Second mortgage statistics

• Percent own

• Bad debt expense
"""

!pip install factor_analyzer

from sklearn.decomposition import FactorAnalysis
from factor_analyzer import FactorAnalyzer

fa=FactorAnalyzer(n_factors=5)
fa.fit_transform(train_DF.select_dtypes(exclude= ('object','category')))
fa.loadings_

"""#Project Task: Week 4

##Data Modeling:

> Keep below considerations while building a linear regression model.

Data Modeling :
1. Variables should have significant impact on predicting Monthly mortgage and owner costs

2. Utilize all predictor variable to start with initial hypothesis

3. R square of 60 percent and above should be achieved

4. Ensure Multi-collinearity does not exist in dependent variables

5. Test if predicted variable is normally distributed

###1. Build a linear Regression model to predict the total monthly expenditure for home mortgages loan. Please refer ‘deplotment_RE.xlsx’. Column hc_mortgage_mean is predicted variable. This is the mean monthly     mortgage and owner costs of specified geographical location. 
Note: Exclude loans from prediction model which have NaN (Not a Number) values for hc_mortgage_mean.
"""

train_DF.columns

train_DF['type'].unique()
type_dict={'type':{'City':1, 
                   'Urban':2, 
                   'Town':3, 
                   'CDP':4, 
                   'Village':5, 
                   'Borough':6}
          }
train_DF.replace(type_dict,inplace=True)

train_DF['type'].unique()

test_DF.replace(type_dict,inplace=True)

test_DF['type'].unique()

feature_cols=['COUNTYID','STATEID','zip_code','type','pop', 'family_mean',
         'second_mortgage', 'home_equity', 'debt','hs_degree',
           'age_median','pct_own', 'married','separated', 'divorced']

x_train=train_DF[feature_cols]
y_train=train_DF['hc_mortgage_mean']

x_test=test_DF[feature_cols]
y_test=test_DF['hc_mortgage_mean']

from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error,mean_squared_error,accuracy_score

x_train.head()

sc=StandardScaler()
x_train_scaled=sc.fit_transform(x_train)
x_test_scaled=sc.fit_transform(x_test)

"""a) Run a model at a Nation level. If the accuracy levels and R square are not satisfactory proceed to below step."""

linereg=LinearRegression()
linereg.fit(x_train_scaled,y_train)

y_pred=linereg.predict(x_test_scaled)

print("Overall R2 score of linear regression model", r2_score(y_test,y_pred))
print("Overall RMSE of linear regression model", np.sqrt(mean_squared_error(y_test,y_pred)))

#The Accuracy and R2 score are good, but still will investigate the model performance at state level

"""b) Run another model at State level. There are 52 states in USA."""

state=train_DF['STATEID'].unique()
state
#Picking a few iDs 20,1,45,6

for i in [20,1,45]:
    print("State ID-",i)
    
    x_train_nation=train_DF[train_DF['COUNTYID']==i][feature_cols]
    y_train_nation=train_DF[train_DF['COUNTYID']==i]['hc_mortgage_mean']
    
    x_test_nation=test_DF[test_DF['COUNTYID']==i][feature_cols]
    y_test_nation=test_DF[test_DF['COUNTYID']==i]['hc_mortgage_mean']
    
    x_train_scaled_nation=sc.fit_transform(x_train_nation)
    x_test_scaled_nation=sc.fit_transform(x_test_nation)
    
    linereg.fit(x_train_scaled_nation,y_train_nation)
    y_pred_nation=linereg.predict(x_test_scaled_nation)
    
    print("Overall R2 score of linear regression model for state,",i,":-" ,r2_score(y_test_nation,y_pred_nation))
    print("Overall RMSE of linear regression model for state,",i,":-" ,np.sqrt(mean_squared_error(y_test_nation,y_pred_nation)))
    print("\n")

# To check the residuals
residuals=y_test-y_pred
residuals

plt.hist(residuals) # Normal distribution of residuals

sns.distplot(residuals)

plt.scatter(residuals,y_pred) 
# Same variance and residuals does not have correlation with predictor
# Independance of residuals

"""##Data Reporting:

2. Create a dashboard in tableau by choosing appropriate chart types and metrics useful for the business. The dashboard must entail the following:

a) Box plot of distribution of average rent by type of place (village, urban, town, etc.).

b) Pie charts to show overall debt and bad debt.

c) Explore the top 2,500 locations where the percentage of households with a second mortgage is the highest and percent ownership is above 10 percent. Visualize using geo-map.

d) Heat map for correlation matrix.

e) Pie chart to show the population distribution across different types of places (village, urban, town etc.)

[Click Here To See DashBoard](https://public.tableau.com/profile/saurabh.zambare#!/vizhome/RealEstate_16057645309010/RealEstate?publish=yes)
"""